{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boppanr/Eco_Tourism_Bot/blob/main/Eco_Tourism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review Genie - A conversation Chatbot for an Echo Tourism Application\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "CkaZ15bkt6SM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS3n2y8wPw5g"
      },
      "source": [
        "## Basics of RAG\n",
        "\n",
        "Before we start coding, lets go over a few questions and get it clarified.\n",
        "\n",
        "### 1. What is RAG?\n",
        "Imagine you're writing an article about climate change, but instead of relying only on what you remember, you search online for recent studies and data to support your writing. RAG works the same way—it combines a powerful LLMs with a search system that retrieves relevant information from an external knowledge source, like a database or documents. This helps generate more accurate and informative responses.\n",
        "\n",
        "### 2. Why is RAG important?\n",
        "RAG is crucial because LLM models, like ChatGPT, can sometimes \"hallucinate\" or provide outdated or incorrect information. By retrieving facts from trusted sources before generating responses, RAG ensures the answers are more reliable, up-to-date, and contextually relevant.\n",
        "\n",
        "### 3. What’s the difference between RAG and a standard LLM chatbot?\n",
        "A standard chatbot relies only on pre-trained knowledge, which may be limited or outdated. RAG-enhanced chatbots, however, actively retrieve fresh, relevant information from external sources, ensuring better accuracy and up-to-date insights.\n",
        "\n",
        "### 4. What are the key components of RAG?\n",
        "RAG consists of two main parts:\n",
        "\n",
        "- Retriever: Finds the most relevant documents or data from a knowledge base (e.g., a search engine or database).\n",
        "- Generator: Uses the retrieved information to produce a coherent and accurate response.\n",
        "\n",
        "### 5. Usecases in real-life\n",
        "- Customer Support: LLM chatbots retrieve knowledge base articles to provide better responses to customer queries.\n",
        "- Healthcare: Doctors can get AI-assisted summaries of patient records and the latest medical research.\n",
        "- Legal Services: Lawyers can search through legal documents and case studies to build stronger cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TYwjqpUdeS5"
      },
      "source": [
        "# Overview of Week 1\n",
        "\n",
        "Key Objectives - our goals are as follows:\n",
        "\n",
        "- Understanding the overall business problem\n",
        "- Identifying the key milestones that we have to close\n",
        "- Understand the apply the key tools that we need here\n",
        "- Building a simple POC app, that will set the groundwork for the subsequent weeks.\n",
        "\n",
        "\n",
        "\n",
        "**Structure of this Notebook**\n",
        "\n",
        "- Basics of RAG.\n",
        "  - The Need for RAG\n",
        "  - Basic RAG app architecture\n",
        "  - Tools for building the RAG app\n",
        "- Building ReviewGenie POC\n",
        "  - Data Preparation\n",
        "  - Vector Store Setup\n",
        "  - Building the chatbot\n",
        "  - Building a simple Gradio UI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-9ZmExzDRva"
      },
      "source": [
        "### Understanding the Limitation of the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRit5A9KCx20"
      },
      "outputs": [],
      "source": [
        "# Importing the OpenAI library to interact with OpenAI's API services.\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKBjz45zDj6r",
        "outputId": "83008d1f-c08e-4a8d-b8e8-e67e778c2a67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os  # Importing the os module to interact with environment variables\n",
        "import getpass  # Importing getpass to securely input sensitive information\n",
        "\n",
        "# Prompting the user to securely enter their OpenAI API key without displaying it on the screen\n",
        "OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7YY7aEjDj9I"
      },
      "outputs": [],
      "source": [
        "# Creating an instance of the OpenAI client using the provided API key.\n",
        "client = OpenAI(api_key= OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtvWPnanDkAj"
      },
      "outputs": [],
      "source": [
        "# Defining the prompt to query the LLM\n",
        "prompt = ''' What was uber's revenue in 2022? '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBFCA_A_DWBp"
      },
      "outputs": [],
      "source": [
        "# Sending a request to the OpenAI API to generate a chat response\n",
        "openai_response = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',  # Specifying the model to use;\n",
        "    # Note: An older model chosen for testing purposes because the cutoff is 2021 whereas prompt is querying details about 2022\n",
        "    messages=[{'role': 'user', 'content': prompt}]  # Creating a structured message for the LLM model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WONjF1M1Xs-k"
      },
      "source": [
        "In the above code, while creating a structured message for the LLM model,  `role `defines the speaker (user input) and `content` contains the actual query stored in the `prompt` variable.\n",
        "\n",
        "We are structuring the input this way because OpenAI's chat models require a specific format to understand and process conversations effectively. Assigning roles like 'user' helps the LLM distinguish between different participants in the conversation, ensuring it provides relevant and context-aware responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u-Y1hWlDij2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7a05c8e2-d152-4665-8149-69bd70cbedcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As of my last update in October 2021, I do not have access to the specific financial data for Uber in 2022. I recommend checking the latest financial reports or news updates for the most accurate information on Uber's revenue in 2022.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Accessing the generated response from the LLM.\n",
        "openai_response.choices[0].message.content\n",
        "\n",
        "\n",
        "# Note:'choices' contains multiple response options, we take the first one ([0]),\n",
        "# 'message' holds the response details, and 'content' extracts the actual text generated by the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJLYV5B_YWGD"
      },
      "source": [
        "### Interpretation:\n",
        "Why is the LLM not able to answer the query?\n",
        "\n",
        "\n",
        "\n",
        "#### Do it yourself:\n",
        "Try changing the model to `gpt-4o-mini` and observe how the output changes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXHhy-hcEMyy"
      },
      "source": [
        "### Making the LLM context-aware\n",
        "\n",
        "Next Step: Let's check Uber's [financial report ](https://s23.q4cdn.com/407969754/files/doc_events/2024/May/06/2023-annual-report.pdf)\n",
        "\n",
        "On Page 54, of the above document it states:\n",
        "\n",
        "\"Revenue was 37.3 billion, up 17% year-over-year. Mobility revenue increased 5.8 billion primarily attributable to an increase in\n",
        "Mobility Gross Bookings of......\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSAk_2EGD1CC"
      },
      "outputs": [],
      "source": [
        "## Let's create the above context for the prompt\n",
        "# Defining a context string with revenue details retrieved from an external source.\n",
        "retrieved_context = '''Revenue was $37.3 billion, up 17% year-over-year. Mobility revenue increased $5.8 billion primarily attributable to an increase in\n",
        "               Mobility Gross Bookings of 31% year-over-year.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irw2unBKFb0M"
      },
      "outputs": [],
      "source": [
        "## Let's modify our prompt now\n",
        "# Creating a prompt by embedding the retrieved context into a question for the LLM.\n",
        "\n",
        "prompt = f\"What was Uber's revenue in 2022? Check in {retrieved_context}\"\n",
        "\n",
        "# Note: The LLM is being asked to analyze the given context and provide Uber's revenue for 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R70ZN0eKFoo7"
      },
      "outputs": [],
      "source": [
        "## Let's ask the LLM again\n",
        "openai_response = client.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [{'role': 'user', 'content': prompt}]                          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09ddo9F1Fs-w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b83114c-bb88-4fb6-b70e-cdbcabbdb4c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As of 2022, Uber's revenue was $37.3 billion.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Accessing the generated response from the LLM model.\n",
        "openai_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LuE9mCnbR7u"
      },
      "source": [
        "### Interpretation:\n",
        "How is the LLM able to answer the same question now?\n",
        "\n",
        "The LLM can now answer the question accurately because the relevant financial data is explicitly provided in the `retrieved_context`, allowing the model to reference it directly instead of relying on its pre-trained knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfpLPTtDG57l"
      },
      "source": [
        "As you saw in the example above, we\n",
        "\n",
        "- **retrieved** the context from an external source\n",
        "- **augmented** our prompt that passes to the LLM, and\n",
        "- **generated** the response\n",
        "\n",
        "This is Retrieval Augmented Generation in a nutshell!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKzjA8hmHtaA"
      },
      "source": [
        "### Basic RAG app architecture\n",
        "\n",
        "In the previous example, we manually retrieved the context from the given file which for all purposes is impractical (duh!!)\n",
        "\n",
        "Therefore, we have to devise a strategy that enables us to:\n",
        "\n",
        "- Take the query from the user\n",
        "- Identify the documents from the external source that might be relevant for the query.\n",
        "- Pass those documents' information as context to the LLM\n",
        "- LLM then generates the final response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msqpnuzuJHYY"
      },
      "source": [
        "To do the above, we can follow a simple standard architecture as shown below (Image source - https://huyenchip.com/2024/07/25/genai-platform.html)\n",
        "\n",
        "<center><img src=\"https://huyenchip.com/assets/pics/genai-platform/3-rag.png\" width=500 height=400/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXm08cxzLsia"
      },
      "source": [
        "As you can see in the above image, the retriever would be the key component of this entire architecture.\n",
        "\n",
        "To build the retriever, we have to follow these steps:\n",
        "\n",
        "- Connect to the document source\n",
        "- Break the documents down to manageable chunks. This is due to the fact that taking in the entire document source for building the context will exceed the token limits of the LLM. This process is also called **Chunking**.\n",
        "- Perform a search for the most relevant chunks based on the given query.\n",
        "- Pass those relevant chunks to the LLM.\n",
        "\n",
        "For performing the search or retrieval process, we will be following an **embedding-based approach.**\n",
        "\n",
        "<center><img src=\"https://cdn.prod.website-files.com/640248e1fd70b63c09bd3d09/653fd23f1565c0c1da063efc_Semantic%20Search%20Text%20Embeddings%20(1).png\" width =500/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MPW_40jNLDn"
      },
      "source": [
        "### Understanding Embedding based approach\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "In the embedding based approach:\n",
        "\n",
        "- We convert the document chunks in the database to vector embeddings and store it in a vector store.\n",
        "\n",
        "- Convert the given user query to an embedding.\n",
        "\n",
        "- Find the document chunks whose vector embeddings are closest to the given query embedding using a vector search algorithm like FAISS (Facebook AI Similarity Search)\n",
        "\n",
        "<center><img src=\"https://miro.medium.com/v2/resize:fit:1400/1*h_btyitJX79d-gFE8RaMQg.png\" width=500/></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7aMQOXtON5U"
      },
      "source": [
        "### Tools for building the RAG App\n",
        "\n",
        "Now that we are familiar with the overall architecture, we can now go ahead and structure the tools that we'll use for the upcoming demonstration:\n",
        "\n",
        "- OpenAI LLM (model - GPT 4o-mini): This will be our primary model for generating the responses\n",
        "- LangChain: Langchain is a powerful framework for orchestrating different layers in the RAG app. We shall use this to build the retriever end-to-end and also connect with other tools for tasks such as\n",
        "    - Chunking - RecursiveCharacterTextSplitter\n",
        "    - Embedding Model - OpenAIEmbeddings\n",
        "    - Vector Search Model - FAISS\n",
        "- Gradio: This will help in building a simple UI at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey558a81QcXs"
      },
      "source": [
        "## Building ReviewGenie POC\n",
        "\n",
        "*A basic chatbot that can answer customer queries*\n",
        "\n",
        "\n",
        "\n",
        "<center><img src=\"https://www.pranathiss.com/static/assets/images/ai-powered-chatBot.webp\" width=500/></center>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQfzsw36du3S"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2OGXS6YSl5Y"
      },
      "source": [
        "### Steps:\n",
        "\n",
        "1. **Data Preparation**:\n",
        "   - Load and process the dataset using pandas\n",
        "\n",
        "2. **Vector Store Setup**:\n",
        "   - Convert product descriptions into embeddings.\n",
        "   - Store embeddings in a vector database.\n",
        "\n",
        "3. **Building the Chatbot**:\n",
        "   - Use LangChain to create an LLM pipeline.\n",
        "   - Develop a simple chatbot to answer product-related queries.\n",
        "\n",
        "4. **Creating a UI**:\n",
        "   - Implement a Gradio-based UI for user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ekric37zU6aV",
        "outputId": "aea68054-f2fc-4218-c641-7647dea50cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchainhub in /usr/local/lib/python3.11/dist-packages (0.1.21)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (2.32.3)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (2.32.4.20250611)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchainhub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchainhub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchainhub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchainhub) (2025.4.26)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.65)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.86.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-openai) (2.11.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-openai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.4.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.65)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.13)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Installing the LangChain Hub package to access and manage pre-built AI chains, prompts, and agents.\n",
        "!pip install langchainhub\n",
        "\n",
        "# Installing the LangChain OpenAI integration to use OpenAI models within LangChain workflows.\n",
        "!pip install langchain-openai\n",
        "\n",
        "# Installing the core LangChain library for building LLM-based applications, including chaining, memory, and retrieval capabilities.\n",
        "!pip install langchain\n",
        "\n",
        "# Installing the community version of LangChain, which includes integrations and tools contributed by the community.\n",
        "!pip install langchain-community\n",
        "\n",
        "# Installing FAISS (Facebook AI Similarity Search) for efficient similarity-based search on text embeddings.\n",
        "!pip install faiss-cpu\n",
        "\n",
        "# Installing Gradio, a framework to create web-based UIs for AI models and applications easily.\n",
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqC1rIL7IyXt"
      },
      "outputs": [],
      "source": [
        "# Importing the KaggleHub library to interact with datasets and models available on Kaggle.\n",
        "import kagglehub\n",
        "\n",
        "# Importing the CSV module for reading and writing CSV files.\n",
        "import csv\n",
        "\n",
        "# Importing pandas for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "\n",
        "# Importing numpy for numerical operations and handling arrays efficiently.\n",
        "import numpy as np\n",
        "\n",
        "# Importing os to interact with the operating system, such as environment variables and file paths.\n",
        "import os\n",
        "\n",
        "# Importing getpass to securely handle user input (e.g., API keys or passwords).\n",
        "import getpass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Vphy17StbK"
      },
      "source": [
        "### STEP 1: Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh9WpGhWRpCU",
        "outputId": "3cd54754-a1b4-419a-9bda-900bd58f66f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XiPvpp1FxKb"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/tourism_resource_dataset.csv\",index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MznuqvAmRx_g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "b24b573b-63d9-4db5-9ce0-60686a21d61e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    location_id  visitor_count  resource_usage_rate  \\\n",
              "timestamp                                                             \n",
              "2024-12-01 00:00:00     LOC_003            808             0.907638   \n",
              "2024-12-01 01:00:00     LOC_001            948             0.974266   \n",
              "2024-12-01 02:00:00     LOC_003            292             0.321912   \n",
              "2024-12-01 03:00:00     LOC_003            592             0.811889   \n",
              "2024-12-01 04:00:00     LOC_001             89             0.936667   \n",
              "2024-12-01 05:00:00     LOC_001            278             0.900152   \n",
              "2024-12-01 06:00:00     LOC_003            597             0.975164   \n",
              "2024-12-01 07:00:00     LOC_002            969             0.832066   \n",
              "2024-12-01 08:00:00     LOC_003            272             0.609609   \n",
              "2024-12-01 09:00:00     LOC_003             55             0.518366   \n",
              "\n",
              "                     temperature  air_quality_index  noise_level  season  \\\n",
              "timestamp                                                                  \n",
              "2024-12-01 00:00:00    19.368864                127    51.506727  summer   \n",
              "2024-12-01 01:00:00    17.404945                 37    55.901717  autumn   \n",
              "2024-12-01 02:00:00    16.366819                113    68.533024  winter   \n",
              "2024-12-01 03:00:00    20.266316                 52    85.301039  autumn   \n",
              "2024-12-01 04:00:00    15.922471                145    52.258779  summer   \n",
              "2024-12-01 05:00:00    26.343416                141    70.581182  spring   \n",
              "2024-12-01 06:00:00    20.676569                134    54.193750  summer   \n",
              "2024-12-01 07:00:00    31.490696                 47    87.103858  spring   \n",
              "2024-12-01 08:00:00    16.484817                 30    62.670967  spring   \n",
              "2024-12-01 09:00:00    19.527030                 66    35.370598  autumn   \n",
              "\n",
              "                     peak_hour_flag  visitor_satisfaction  sensor_noise_flag  \\\n",
              "timestamp                                                                      \n",
              "2024-12-01 00:00:00               0              5.502615                  1   \n",
              "2024-12-01 01:00:00               0              4.736401                  0   \n",
              "2024-12-01 02:00:00               1              2.522827                  0   \n",
              "2024-12-01 03:00:00               1              2.687745                  1   \n",
              "2024-12-01 04:00:00               1              1.094965                  1   \n",
              "2024-12-01 05:00:00               1              1.822836                  0   \n",
              "2024-12-01 06:00:00               1              6.011548                  0   \n",
              "2024-12-01 07:00:00               0              9.026858                  0   \n",
              "2024-12-01 08:00:00               0              3.584483                  0   \n",
              "2024-12-01 09:00:00               0              3.949725                  1   \n",
              "\n",
              "                     resource_prediction resource_allocation  t_sne_dim1  \\\n",
              "timestamp                                                                  \n",
              "2024-12-01 00:00:00             0.857819                high   -4.576337   \n",
              "2024-12-01 01:00:00             0.961133                high  -28.314085   \n",
              "2024-12-01 02:00:00             0.306956                 low    1.329948   \n",
              "2024-12-01 03:00:00             0.701945              medium  -11.921675   \n",
              "2024-12-01 04:00:00             0.512834              medium   -6.068825   \n",
              "2024-12-01 05:00:00             0.589076              medium  -30.603321   \n",
              "2024-12-01 06:00:00             0.786082              medium   -1.928188   \n",
              "2024-12-01 07:00:00             0.900533                high  -22.911472   \n",
              "2024-12-01 08:00:00             0.440805                 low   15.556006   \n",
              "2024-12-01 09:00:00             0.286683                 low   -4.017957   \n",
              "\n",
              "                     t_sne_dim2  \n",
              "timestamp                        \n",
              "2024-12-01 00:00:00    0.582736  \n",
              "2024-12-01 01:00:00   20.022820  \n",
              "2024-12-01 02:00:00    5.881103  \n",
              "2024-12-01 03:00:00   20.376535  \n",
              "2024-12-01 04:00:00   -4.793058  \n",
              "2024-12-01 05:00:00    2.003588  \n",
              "2024-12-01 06:00:00  -13.306153  \n",
              "2024-12-01 07:00:00    6.416325  \n",
              "2024-12-01 08:00:00    3.516501  \n",
              "2024-12-01 09:00:00  -26.208872  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3422c31-0150-474a-b695-d3c91a5f1466\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>location_id</th>\n",
              "      <th>visitor_count</th>\n",
              "      <th>resource_usage_rate</th>\n",
              "      <th>temperature</th>\n",
              "      <th>air_quality_index</th>\n",
              "      <th>noise_level</th>\n",
              "      <th>season</th>\n",
              "      <th>peak_hour_flag</th>\n",
              "      <th>visitor_satisfaction</th>\n",
              "      <th>sensor_noise_flag</th>\n",
              "      <th>resource_prediction</th>\n",
              "      <th>resource_allocation</th>\n",
              "      <th>t_sne_dim1</th>\n",
              "      <th>t_sne_dim2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2024-12-01 00:00:00</th>\n",
              "      <td>LOC_003</td>\n",
              "      <td>808</td>\n",
              "      <td>0.907638</td>\n",
              "      <td>19.368864</td>\n",
              "      <td>127</td>\n",
              "      <td>51.506727</td>\n",
              "      <td>summer</td>\n",
              "      <td>0</td>\n",
              "      <td>5.502615</td>\n",
              "      <td>1</td>\n",
              "      <td>0.857819</td>\n",
              "      <td>high</td>\n",
              "      <td>-4.576337</td>\n",
              "      <td>0.582736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01 01:00:00</th>\n",
              "      <td>LOC_001</td>\n",
              "      <td>948</td>\n",
              "      <td>0.974266</td>\n",
              "      <td>17.404945</td>\n",
              "      <td>37</td>\n",
              "      <td>55.901717</td>\n",
              "      <td>autumn</td>\n",
              "      <td>0</td>\n",
              "      <td>4.736401</td>\n",
              "      <td>0</td>\n",
              "      <td>0.961133</td>\n",
              "      <td>high</td>\n",
              "      <td>-28.314085</td>\n",
              "      <td>20.022820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01 02:00:00</th>\n",
              "      <td>LOC_003</td>\n",
              "      <td>292</td>\n",
              "      <td>0.321912</td>\n",
              "      <td>16.366819</td>\n",
              "      <td>113</td>\n",
              "      <td>68.533024</td>\n",
              "      <td>winter</td>\n",
              "      <td>1</td>\n",
              "      <td>2.522827</td>\n",
              "      <td>0</td>\n",
              "      <td>0.306956</td>\n",
              "      <td>low</td>\n",
              "      <td>1.329948</td>\n",
              "      <td>5.881103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01 03:00:00</th>\n",
              "      <td>LOC_003</td>\n",
              "      <td>592</td>\n",
              "      <td>0.811889</td>\n",
              "      <td>20.266316</td>\n",
              "      <td>52</td>\n",
              "      <td>85.301039</td>\n",
              "      <td>autumn</td>\n",
              "      <td>1</td>\n",
              "      <td>2.687745</td>\n",
              "      <td>1</td>\n",
              "      <td>0.701945</td>\n",
              "      <td>medium</td>\n",
              "      <td>-11.921675</td>\n",
              "      <td>20.376535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01 04:00:00</th>\n",
              "      <td>LOC_001</td>\n",
              "      <td>89</td>\n",
              "      <td>0.936667</td>\n",
              "      <td>15.922471</td>\n",
              "      <td>145</td>\n",
              "      <td>52.258779</td>\n",
              "      <td>summer</td>\n",
              "      <td>1</td>\n",
              "      <td>1.094965</td>\n",
              "      <td>1</td>\n",
              "      <td>0.512834</td>\n",
              "      <td>medium</td>\n",
              "      <td>-6.068825</td>\n",
              "      <td>-4.793058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01 05:00:00</th>\n",
              "      <td>LOC_001</td>\n",
              "      <td>278</td>\n",
              "      <td>0.900152</td>\n",
              "      <td>26.343416</td>\n",
              "      <td>141</td>\n",
              "      <td>70.581182</td>\n",
              "      <td>spring</td>\n",
              "      <td>1</td>\n",
              "      <td>1.822836</td>\n",
              "      <td>0</td>\n",
              "      <td>0.589076</td>\n",
              "      <td>medium</td>\n",
              "      <td>-30.603321</td>\n",
              "      <td>2.003588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01 06:00:00</th>\n",
              "      <td>LOC_003</td>\n",
              "      <td>597</td>\n",
              "      <td>0.975164</td>\n",
              "      <td>20.676569</td>\n",
              "      <td>134</td>\n",
              "      <td>54.193750</td>\n",
              "      <td>summer</td>\n",
              "      <td>1</td>\n",
              "      <td>6.011548</td>\n",
              "      <td>0</td>\n",
              "      <td>0.786082</td>\n",
              "      <td>medium</td>\n",
              "      <td>-1.928188</td>\n",
              "      <td>-13.306153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01 07:00:00</th>\n",
              "      <td>LOC_002</td>\n",
              "      <td>969</td>\n",
              "      <td>0.832066</td>\n",
              "      <td>31.490696</td>\n",
              "      <td>47</td>\n",
              "      <td>87.103858</td>\n",
              "      <td>spring</td>\n",
              "      <td>0</td>\n",
              "      <td>9.026858</td>\n",
              "      <td>0</td>\n",
              "      <td>0.900533</td>\n",
              "      <td>high</td>\n",
              "      <td>-22.911472</td>\n",
              "      <td>6.416325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01 08:00:00</th>\n",
              "      <td>LOC_003</td>\n",
              "      <td>272</td>\n",
              "      <td>0.609609</td>\n",
              "      <td>16.484817</td>\n",
              "      <td>30</td>\n",
              "      <td>62.670967</td>\n",
              "      <td>spring</td>\n",
              "      <td>0</td>\n",
              "      <td>3.584483</td>\n",
              "      <td>0</td>\n",
              "      <td>0.440805</td>\n",
              "      <td>low</td>\n",
              "      <td>15.556006</td>\n",
              "      <td>3.516501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-12-01 09:00:00</th>\n",
              "      <td>LOC_003</td>\n",
              "      <td>55</td>\n",
              "      <td>0.518366</td>\n",
              "      <td>19.527030</td>\n",
              "      <td>66</td>\n",
              "      <td>35.370598</td>\n",
              "      <td>autumn</td>\n",
              "      <td>0</td>\n",
              "      <td>3.949725</td>\n",
              "      <td>1</td>\n",
              "      <td>0.286683</td>\n",
              "      <td>low</td>\n",
              "      <td>-4.017957</td>\n",
              "      <td>-26.208872</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3422c31-0150-474a-b695-d3c91a5f1466')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a3422c31-0150-474a-b695-d3c91a5f1466 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a3422c31-0150-474a-b695-d3c91a5f1466');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a2971b89-c71c-4afc-921a-80f3032d1d3c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2971b89-c71c-4afc-921a-80f3032d1d3c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a2971b89-c71c-4afc-921a-80f3032d1d3c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"2024-12-22 17:00:00\",\n          \"2024-12-31 17:00:00\",\n          \"2024-12-31 20:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"LOC_003\",\n          \"LOC_001\",\n          \"LOC_002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"visitor_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 281,\n        \"min\": 50,\n        \"max\": 999,\n        \"num_unique_values\": 605,\n        \"samples\": [\n          359,\n          727,\n          866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resource_usage_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19918270058496096,\n        \"min\": 0.3001318796896073,\n        \"max\": 0.9979547099571632,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.4238210016006981,\n          0.42143605198617,\n          0.4247767268332491\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperature\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.758527362493416,\n        \"min\": 15.008855628744158,\n        \"max\": 34.99427608311828,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          19.09962419053633,\n          32.009432295216754,\n          16.037905990368216\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"air_quality_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37,\n        \"min\": 20,\n        \"max\": 149,\n        \"num_unique_values\": 130,\n        \"samples\": [\n          98,\n          73,\n          67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"noise_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19.91609819200633,\n        \"min\": 30.01305697194642,\n        \"max\": 99.82626918208724,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          92.05517686493316,\n          55.05728361729352,\n          39.6857740621733\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"autumn\",\n          \"spring\",\n          \"summer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"peak_hour_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"visitor_satisfaction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.5542907651556304,\n        \"min\": 1.014410687941259,\n        \"max\": 9.9977640708614,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          5.896269687651181,\n          5.578227466564384\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sensor_noise_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resource_prediction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17456047598052193,\n        \"min\": 0.1843013987520383,\n        \"max\": 0.976714556986186,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.553410500800349,\n          0.643718025993085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"resource_allocation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"high\",\n          \"low\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"t_sne_dim1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.179541830454724,\n        \"min\": -34.30869,\n        \"max\": 36.515007,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          9.565732,\n          22.80191\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"t_sne_dim2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15.797633471480655,\n        \"min\": -38.91482,\n        \"max\": 33.65287,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          -14.902231,\n          1.6327542\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Viewing the data\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-3RpjmUS21M"
      },
      "source": [
        "**Constructing the text data**\n",
        "\n",
        "It's useful to use both `Title` and `Description`. To help downstream models understand which content is title and which content is description, we will add a prefix explaining which section is title and which is description. So each row should look like\n",
        "\n",
        "```\n",
        "Title\n",
        "{Title}\n",
        "Description\n",
        "{Description}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRjed6aXUo6j"
      },
      "source": [
        "### STEP 2: Vector Store Setup\n",
        "\n",
        "Let's try to get a few of the basic questions answered about vector stores before we start using it.\n",
        "\n",
        "### What is a vector store?\n",
        "A vector store is a specialized database that stores data in the form of numerical vectors, allowing efficient searching and retrieval based on similarity rather than exact matches.\n",
        "\n",
        "### Why do we need a vector store?\n",
        "Traditional databases rely on exact keyword matches, which can miss relevant information. A vector store helps find similar content by understanding relationships and meaning in data.\n",
        "\n",
        "### How does a vector store work?\n",
        "It converts text, images, or other data into numerical vectors using ML\n",
        " models, then stores these vectors and retrieves similar ones using techniques like cosine similarity.\n",
        "\n",
        "### How does a vector store improve search results?\n",
        "It enables searches based on meaning rather than just keywords, providing more relevant results even if the exact terms don't match.\n",
        "\n",
        "### What are some popular vector store tools?\n",
        "- FAISS (Facebook AI Similarity Search)\n",
        "- Pinecone\n",
        "- Weaviate\n",
        "- Chroma\n",
        "\n",
        "### What is an embedding, and how does it relate to a vector store?\n",
        "An embedding is a numerical representation of data (e.g., text, image) that captures its meaning. These embeddings are stored in a vector store for efficient retrieval.\n",
        "\n",
        "\n",
        "Our next step is\n",
        "-  to convert the `product_description` to chunks\n",
        "-  convert each chunk to embedding\n",
        "-  store it in vector store for searching\n",
        "\n",
        "As discussed earlier we shall use `LangChain` to perform these steps.\n",
        "\n",
        "LangChain is a framework that helps developers build applications powered by large language models (LLMs) like GPT by providing tools for various tasks to be carried out like retrieving relevant information from databases, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwFdqweOT2xI"
      },
      "outputs": [],
      "source": [
        "# Importing RecursiveCharacterTextSplitter from LangChain for chunking large text into smaller, manageable pieces.\n",
        "# This helps in optimizing text for processing and retrieval.\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Importing OpenAIEmbeddings from LangChain to generate numerical vector representations (embeddings) of text.\n",
        "# These embeddings capture the semantic meaning of the text for efficient similarity searches.\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Importing FAISS (Facebook AI Similarity Search) from LangChain's community package.\n",
        "# FAISS is used for storing and retrieving embeddings efficiently by finding similar vectors.\n",
        "from langchain_community.vectorstores import FAISS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X3hxmTTUsr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa7ad0e-c0d2-46c2-bb83-2477eb31e516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Location LOC_003 had 808 visitors. Resource usage rate: 0.9076380257726048%. Temperature: 19.368863657666143°C, AQI: 127, Noise level: 51.50672732934044 dB. Season: summer, Peak hour: 0. Visitor satisfaction: 5.502614668103616, Resource prediction:'\n"
          ]
        }
      ],
      "source": [
        "# Setting the OpenAI API key as an environment variable.\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "# Convert each row into a readable string for embeddings\n",
        "texts = df.apply(lambda row: (\n",
        "    f\"Location {row['location_id']} had {row['visitor_count']} visitors. \"\n",
        "    f\"Resource usage rate: {row['resource_usage_rate']}%. \"\n",
        "    f\"Temperature: {row['temperature']}°C, AQI: {row['air_quality_index']}, Noise level: {row['noise_level']} dB. \"\n",
        "    f\"Season: {row['season']}, Peak hour: {row['peak_hour_flag']}. \"\n",
        "    f\"Visitor satisfaction: {row['visitor_satisfaction']}, Resource prediction: {row['resource_prediction']} units.\"\n",
        "), axis=1).tolist()\n",
        "\n",
        "# Split text into documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=250,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "documents = text_splitter.create_documents(texts)\n",
        "\n",
        "# Show a sample document\n",
        "print(documents[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Explanation:\n",
        "The above code initializes a `RecursiveCharacterTextSplitter` to break down product_description into smaller text chunks of 250 characters each, with a 20-character overlap to preserve context between chunks. The `create_documents` function processes the text list and generates structured document chunks for efficient retrieval and analysis.\n",
        "\n",
        "### Why do we need overlap?\n",
        "Overlap is needed to ensure continuity and preserve context between chunks, preventing important information from being cut off at chunk boundaries. This helps LLMs better understand the text when processing each chunk independently, improving retrieval accuracy and response quality."
      ],
      "metadata": {
        "id": "MqtfA0tXh7CL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptE8XpNLWD--"
      },
      "outputs": [],
      "source": [
        "# Create an embedding model using LangChain.\n",
        "# One option is using https://python.langchain.com/docs/integrations/text_embedding/openai/\n",
        "# See https://python.langchain.com/docs/integrations/text_embedding/ for a list of available embedding models on LangChain\n",
        "embeddings = OpenAIEmbeddings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSlO9vblWGDt"
      },
      "outputs": [],
      "source": [
        "# Create a vector store using the created chunks and the embeddings model\n",
        "vector = FAISS.from_documents(documents, embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What have we done so far?\n",
        "1. Data Preparation: Extracted the product description data\n",
        "2. Data Chunking: Converted the entire data into multiple manageable chunks\n",
        "3. Chunks to Embeddings: Converted the broken down chunks into embeddings\n",
        "4. Storage in a Vector DB: Stored the resulting embeddings of chunks in a vector store for effective retrieval.\n",
        "\n",
        "\n",
        "### What is remaining?\n",
        "- Building the chatbot\n",
        "- Building the Gradio UI"
      ],
      "metadata": {
        "id": "nNhCKF9Nig_n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c24S9JsfWSa7"
      },
      "source": [
        "### STEP 3: Building the chatbot\n",
        "\n",
        "Now that we have converted the documents to embeddings, our next step is to\n",
        "- build a retriever that uses the vector store to retrieve the documents\n",
        "- create a prompt template that contains the augmented context using the retrieved documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nElsMdbeW4l1"
      },
      "outputs": [],
      "source": [
        "# Importing ChatOpenAI from LangChain to interact with OpenAI's language models, such as GPT, for generating responses.\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Importing ChatPromptTemplate to create structured prompts for the chatbot, ensuring consistent interactions with the LLM model.\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Importing OpenAIEmbeddings to convert text data into numerical vector representations for similarity search and retrieval.\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Importing ChatPromptTemplate again (duplicate import, should be removed to avoid redundancy).\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Importing create_stuff_documents_chain to combine and process retrieved documents for meaningful AI-generated responses.\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Importing create_retrieval_chain to build a chain that retrieves relevant documents from a vector store and generates AI responses.\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "# Importing StrOutputParser from LangChain to parse the output\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code Explanation:\n",
        "- `ChatOpenAI` – Used to access OpenAI models for chatbot functionality.\n",
        "- `ChatPromptTemplate` – Helps structure queries to ensure better responses.\n",
        "- `OpenAIEmbeddings` – Converts text into vector form for similarity-based retrieval.\n",
        "- `create_stuff_documents_chain` – Combines retrieved documents meaningfully before passing to the LLM.\n",
        "- `create_retrieval_chain` – Automates the process of retrieving and utilizing relevant content for AI responses.\n",
        "- `StrOutputParser` - For processing the output of language models, ensuring that the output is returned as a plain string"
      ],
      "metadata": {
        "id": "xVChvJAijx-1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dJXzILCXStu"
      },
      "outputs": [],
      "source": [
        "# Initializing the ChatOpenAI model to interact with OpenAI's GPT model.\n",
        "llm = ChatOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"], model = 'gpt-4o-mini')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the output parser to process and format the model's response into a readable string format.\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Creating a prompt template that instructs the LLM to act as a customer service agent.\n",
        "# The prompt takes two parameters:\n",
        "#   1. {context} - Relevant information retrieved from the document store.\n",
        "#   2. {input} - The user's question.\n",
        "# The model is instructed to base its answer solely on the provided context.\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Answer the following question based only on the provided context:\n",
        "\n",
        "    <context>\n",
        "    {context}\n",
        "    </context>\n",
        "\n",
        "    Question: {input}\"\"\",\n",
        "    output_parser= output_parser # The output parser ensures that the response is returned in a structured string format.\n",
        ")\n",
        "\n",
        "# Creating a document processing chain using the LLM and the defined prompt\n",
        "# template.\n",
        "# This chain takes a list of retrieved documents and passes them as context to\n",
        "# the model for generating responses.\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# Alternative chain creation method:\n",
        "# Using the \"|\" (pipe) operator to link the prompt with the language model (llm),\n",
        "# meaning the input first goes to the prompt and then to the model for\n",
        "# response generation.\n",
        "# document_chain = prompt | llm\n",
        "print(output_parser)\n"
      ],
      "metadata": {
        "id": "dPJ3yFeEk9EN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86368bb1-773d-4ae2-d792-6f29ea84f3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code Explanation:\n",
        "- A structured prompt is created using `ChatPromptTemplate` to guide LLMs in answering questions based solely on provided context.\n",
        "- The prompt includes placeholders `{context}` and `{input}` to dynamically inject relevant information.\n",
        "- `StrOutputParser()` ensures that the LLM's response is formatted as plain text for easy processing and display.\n",
        "- `create_stuff_documents_chain(llm, prompt)` combines the language model (LLM) with the prompt to form a processing chain. This chain takes retrieved documents as input and generates AI-driven responses.\n",
        "- Alternate way:  `prompt | llm` is a more concise way to chain the prompt and model, achieving the same functionality with a cleaner syntax."
      ],
      "metadata": {
        "id": "__HKSf_Umiq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever from the vector store for fetching relevant documents\n",
        "# See https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/\n",
        "retriever = vector.as_retriever()\n",
        "\n",
        "# Create a retrieval chain that first retrieves relevant documents and then processes them using the document chain\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
      ],
      "metadata": {
        "id": "VMiWghjrl5oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code Explanation:\n",
        "- The `vector.as_retriever()` converts the vector store into a retriever to find documents based on query similarity.\n",
        "- The `create_retrieval_chain()` connects the retriever with the document processing pipeline, ensuring LLM receives relevant context before generating responses.\n",
        "\n",
        "This setup enables LLM to provide accurate answers by first retrieving and then processing relevant documents."
      ],
      "metadata": {
        "id": "xY3X-YJZmT3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUqxhEW3XcJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676cfea6-937c-4eeb-c30c-901a96a55ae7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what location has the top visitors?',\n",
              " 'context': [Document(id='25ea23ef-8480-478d-b48e-1cdc015d988d', metadata={}, page_content='Location LOC_001 had 220 visitors. Resource usage rate: 0.8191984430590509%. Temperature: 16.901060910653854°C, AQI: 88, Noise level: 35.66676152809817 dB. Season: summer, Peak hour: 1. Visitor satisfaction: 1.5239576086780569, Resource prediction:'),\n",
              "  Document(id='7eaad1da-68f0-406c-82dd-9504b8eff6ec', metadata={}, page_content='Location LOC_001 had 666 visitors. Resource usage rate: 0.962925191417106%. Temperature: 34.80723563996744°C, AQI: 59, Noise level: 86.35817959213244 dB. Season: summer, Peak hour: 1. Visitor satisfaction: 6.003208153668883, Resource prediction:'),\n",
              "  Document(id='9e4e339f-8aea-4bd7-aa7b-2ffdb2229e00', metadata={}, page_content='Location LOC_001 had 540 visitors. Resource usage rate: 0.8468192966508434%. Temperature: 17.739981261073428°C, AQI: 118, Noise level: 38.30624301321772 dB. Season: summer, Peak hour: 1. Visitor satisfaction: 8.362937514895735, Resource prediction:'),\n",
              "  Document(id='a533a491-3724-4932-8f19-1950e86a4791', metadata={}, page_content='Location LOC_001 had 970 visitors. Resource usage rate: 0.362188171561295%. Temperature: 34.54208466103147°C, AQI: 68, Noise level: 80.72873779832771 dB. Season: summer, Peak hour: 1. Visitor satisfaction: 6.70417278393456, Resource prediction:')],\n",
              " 'answer': 'Location LOC_001 has the top visitors with 970 visitors.'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# Invoking the retrieval chain to process the user's query.\n",
        "# The query \"what are some of the best shoes available?\" is passed as input.\n",
        "# The retrieval chain first fetches relevant product descriptions from the vector store,\n",
        "# then processes them using the document chain to generate a meaningful LLM response.\n",
        "retrieval_chain.invoke({\"input\": \"what location has the top visitors?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqNvod6GXfjx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "046fbace-dc29-4584-a6fd-9d58ccb26498"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The season with the top visitors is summer, with 970 visitors recorded.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# Fetching the final answer from the retrieval chain by invoking it with a user query.\n",
        "# The ['answer'] key extracts the final LLM-generated answer from the response dictionary.\n",
        "retrieval_chain.invoke({\"input\": \"what season has the top visitors?\"})['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we got the answer! But, the formatting is not very good, right? Lets create a simple UI for our bot."
      ],
      "metadata": {
        "id": "2tMQ3SGUnoad"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvDsSk-bYSwS"
      },
      "source": [
        "### STEP 4: Building a simple Gradio UI\n",
        "\n",
        "Gradio is an open-source Python library that makes it easy to build interactive user interfaces for machine learning models, APIs, and data science workflows. It allows developers to create shareable web-based UIs with just a few lines of code.\n",
        "\n",
        "To build the gradio app we'll utilize the following steps:\n",
        "\n",
        "- Modularize the entire RAG pipeline using a single function\n",
        "- Create the building blocks for the UI.\n",
        "- Connect the UI with the function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the user query and return formatted product names\n",
        "def final_response(user_query):\n",
        "    # Invoking the retrieval chain with the user's query to fetch relevant product information\n",
        "    response = retrieval_chain.invoke({\"input\": user_query})['answer']\n",
        "\n",
        "    # Creating a prompt to instruct the LLM to format the response properly\n",
        "    # The prompt asks the LLM to extract only product names from the retrieved response\n",
        "    prompt = f\"Format the responses properly in {response}. Just return the product names, no other text\"\n",
        "\n",
        "    # Sending the formatted prompt to the GPT-4o-mini model for processing\n",
        "    openai_response = client.chat.completions.create(\n",
        "        model='gpt-4o-mini',  # Using GPT-4o-mini model for response generation\n",
        "        messages=[{'role': 'user', 'content': prompt}]  # Providing the prompt to the model\n",
        "    )\n",
        "\n",
        "    # Extracting and returning the LLM-generated response containing only the product names\n",
        "    return openai_response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "PcAbbKmKoHZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjS_RUh6ZstZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "0c03f3c7-e7c8-4412-fda9-f572eb22d50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://34d73e6e24c4b7bfe8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://34d73e6e24c4b7bfe8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Importing the Gradio library to create a simple web-based user interface\n",
        "import gradio as gr\n",
        "\n",
        "# Creating the Gradio interface for the product recommendation system\n",
        "# Creating the Gradio interface for the product recommendation system\n",
        "app = gr.Interface(\n",
        "    fn=final_response,        # The function that processes user input and returns recommendations\n",
        "    inputs=\"text\",            # Input component: a text box for users to enter their query\n",
        "    outputs=\"text\",           # Output component: a text box to display the AI-generated response\n",
        "    title=\"Review Genie\",     # The title of the web interface\n",
        "    description=\"Type your question below to get the recommendations\",# A brief description displayed to users\n",
        "    theme=\"Ocean\",\n",
        "    allow_flagging=\"never\"    # Disabling the flagging feature to remove the \"Flag\" button\n",
        ")\n",
        "\n",
        "# Launching the Gradio app to start the interface and make it accessible via web browser\n",
        "app.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps for Experimentation\n",
        "\n",
        "In the above demonstration, we have created a very basic GUI-based RAG app. Before our next class, you are recommende to experiment the following\n",
        "\n",
        "- Try asking 10-15 questions and check the accuracy of the response.\n",
        "- What would be a strategy that you might follow to evaluate the app's overall responses?\n",
        "- Ask a few out-of-the-box questions (like \"What is the weather today in Seattle?\")\n",
        "- Try asking the same question multiple times with a different wording\n",
        "- In the same chat history, ask questions which are related to the response that was just provided.\n",
        "- Use a different dataset (perhaps in a different format like pdf) and see how the architecture might change.\n",
        "\n",
        "<hr> <hr>"
      ],
      "metadata": {
        "id": "5BG7LdhktfTq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}